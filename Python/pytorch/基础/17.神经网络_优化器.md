### 步骤

1. 构造优化器
   
   1. 选择优化器算法
   
   ```python
   optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
   optimizer = optim.Adam([var1, var2], lr=0.0001)
   ```
   
2. 调用优化器的  `step`  方法
   
   1. 应用之前得到的梯度对参数进行分析
   
   ```python
    optimizer.step()
   ```
   
   



---



### 教学

- ```python
  optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
  ```
  - `model.parameters()`  放入模型的参数，必备参数，有了参数才知道对什么进行调整
  - `lr`  学习速率，几乎每一个优化器都有的参数
    - 设置不能太大和太小，太大模型训练不稳定，太小训练速度慢
    - 刚开始采用大的学习速率进行学习，后面采用小的学习速率进行学习
  - `momentum`  这个优化器对应的自己特定的参数

- 官方示例解析

  ```python
  for input, target in dataset:
      optimizer.zero_grad()	# 上一次计算出的梯度清零，防止上一次计算造成的影响
      output = model(input)	# 输入经过一个模型得到了输出
      loss = loss_fn(output, target)	# 输出与target计算出loss误差
      loss.backward()		# 得到每个参数对应的梯度
      optimizer.step()	# 卷积核等的参数进行优化调整
  ```



- 学习实例代码

  ```python
  import torch
  import torch.nn as nn
  import torchvision
  from torch import optim
  from torch.utils.data import DataLoader
  from torch.utils.tensorboard import SummaryWriter
  
  dataset = torchvision.datasets.CIFAR10(root="../dataset_download", train=False,
                                         transform=torchvision.transforms.ToTensor(), download=True)
  dataLoader = DataLoader(dataset, batch_size=1, shuffle=False, drop_last=True)
  
  
  class OptimModule(nn.Module):
      def __init__(self):
          super(OptimModule, self).__init__()
          self.model = nn.Sequential(
              nn.Conv2d(3, 32, 5, padding=2),
              nn.MaxPool2d(2),
              nn.Conv2d(32, 32, 5, padding=2),
              nn.MaxPool2d(2),
              nn.Conv2d(32, 64, 5, padding=2),
              nn.MaxPool2d(2),
              nn.Flatten(),
              nn.Linear(1024, 64),
              nn.Linear(64, 10)
          )
  
      def forward(self, input):
          output = self.model(input)
          return output
  
  
  optimModule = OptimModule()
  loss_cross = torch.nn.CrossEntropyLoss()
  
  # 构造优化器，选择优化器算法
  optimizer = optim.SGD(optimModule.parameters(), lr=0.01)
  for epoch in range(10):
      loss_sum = 0.0
      for data in dataLoader:
          imgs, targets = data
          outputs = optimModule(imgs)
          result_cross = loss_cross(outputs, targets)  # 计算偏差
          optimizer.zero_grad()  # 梯度归 0	>>>>>>>>>>>>>>>>>>>>>>>>	缺少此行，每次梯度会不断累加
          result_cross.backward()  # 计算梯度
          optimizer.step()  # 对参数进行优化
          loss_sum = loss_sum + result_cross
      print(loss_sum)
  ```

- 学习速率的调整

  ```python
  torch.optim.lr_scheduler.StepLR(optimizer, step_size, gamma=0.1, last_epoch=-1, verbose=False)
  ```

  - 官方示例

    ```python
    >>> # Assuming optimizer uses lr = 0.05 for all groups
    >>> # lr = 0.05     if epoch < 30
    >>> # lr = 0.005    if 30 <= epoch < 60
    >>> # lr = 0.0005   if 60 <= epoch < 90
    >>> # ...
    >>> scheduler = StepLR(optimizer, step_size=30, gamma=0.1)	# 每30次乘以0.1
    >>> for epoch in range(100):
    >>>     train(...)
    >>>     validate(...)
    >>>     scheduler.step()
    ```

  - 学习实例代码

    ```python
    # 构造优化器，选择优化器算法
    # 前面代码省略
    optimizer = optim.SGD(optimModule.parameters(), lr=0.001)
    scheduler = StepLR(optimizer, step_size=30, gamma=0.1)	# >>>>>>>>>>>>>>>>>> 添加此行
    for epoch in range(10):
        loss_sum = 0.0
        for data in dataLoader:
            imgs, targets = data
            outputs = optimModule(imgs)
            result_cross = loss_cross(outputs, targets)  # 计算偏差
            optimizer.zero_grad()  # 梯度归 0
            result_cross.backward()  # 计算梯度
            scheduler.step()  # 对参数进行优化>>>>>>>>>>>>>>>>>>>> 此处需要替换成 scheduler
            loss_sum = loss_sum + result_cross
        print(loss_sum)
    ```

    





---

### 算法

