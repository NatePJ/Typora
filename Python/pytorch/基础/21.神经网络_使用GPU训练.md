### 本地  GPU  训练

#### 方法一：cuda()

- 可使用  `GPU`  对象：该对象对应有相应的  `cuda()`  方法，即可以使用  `GPU`
  - 模型
  - 数据  *(加载后的数据)*
  - 损失函数

- 不能使用  `GPU`  对象

  - 加载前数据集
  - 优化器

- 学习实例代码

  ```python
  import time
  import torch.optim
  import torchvision
  from torch import nn
  from torch.utils.data import DataLoader
  from torch.utils.tensorboard import SummaryWriter
  from model.TrainModule import TrainModule
  
  # 准备数据集
  train_data = torchvision.datasets.CIFAR10(root="../dataset_download", train=True,
                                            transform=torchvision.transforms.ToTensor(), download=True)
  test_data = torchvision.datasets.CIFAR10(root="../dataset_download", train=False,
                                           transform=torchvision.transforms.ToTensor(), download=True)
  
  # 获取数据集的长度
  train_data_size = len(train_data)
  test_data_size = len(test_data)
  print("训练数据集的长度为: {}".format(train_data_size))
  print("测试数据集的长度为: {}".format(test_data_size))
  
  # 利用 DataLoader 加载数据集
  train_data_loader = DataLoader(train_data, batch_size=64)
  test_data_loader = DataLoader(test_data, batch_size=64)
  
  # 创建网络模型
  trainModule = TrainModule()
  if torch.cuda.is_available():
      trainModule = trainModule.cuda()  # 使用 GPU 加速
  
  # 损失函数
  loss_fun = nn.CrossEntropyLoss()
  if torch.cuda.is_available():
      loss_fun = loss_fun.cuda()  # 使用 GPU 加速
  
  # 优化器
  learning_rate = 0.01
  optimizer = torch.optim.SGD(trainModule.parameters(), lr=learning_rate)
  
  # 设置训练网络的一些参数
  # 记录训练次数
  total_train_step = 0
  # 记录测试的次数
  total_test_step = 0
  # 训练的轮数
  epoch = 10
  # 添加tensorboard
  writer = SummaryWriter("../logs_train")
  
  # 开始训练
  # 测试 GPU 加速
  start_time = time.time()
  for i in range(epoch):
      print("------------------- 第 {} 轮训练开始了 ---------------------".format(i + 1))
      # 训练步骤开始
      for data in train_data_loader:
          imgs, targets = data
          if torch.cuda.is_available():
              imgs = imgs.cuda()  # 使用 GPU 加速
              targets = targets.cuda()  # 使用 GPU 加速
          output = trainModule(imgs)
          # 得到误差
          loss = loss_fun(output, targets)
          # 优化参数
          optimizer.zero_grad()
          loss.backward()
          optimizer.step()
          # 记录数据
          total_train_step = total_train_step + 1
          # 100 次打印输出
          if total_train_step % 100 == 0:
              print("第 {} 次训练，Loss: {}".format(total_train_step, loss.item()))
              writer.add_scalar("train_loss", loss.item(), total_train_step)
  
      # 测试步骤开始
      # 添加测试正确率
      total_accuracy = 0
      total_test_loss = 0
      with torch.no_grad():
          for data in test_data_loader:
              imgs, targets = data
              if torch.cuda.is_available():
                  imgs = imgs.cuda()  # 使用 GPU 加速
                  targets = targets.cuda()  # 使用 GPU 加速
              output = trainModule(imgs)
              loss = loss_fun(output, targets)
              total_test_loss = total_test_loss + loss.item()
              # 测试正确率
              accuracy = (targets == output.argmax(1)).sum()
              total_accuracy = total_accuracy + accuracy
      print("整体测试集上的 Loss: {}".format(total_test_loss))
      print("整体测试集上的正确率 {}".format(total_accuracy / test_data_size))
      total_test_step = total_test_step + 1
      writer.add_scalar("total_test_loss", total_test_loss, total_test_step)
      writer.add_scalar("total_test_accuracy", total_accuracy / test_data_size, total_test_step)
  
      # 保存每一轮训练的结果
      torch.save(trainModule, "../save_train_model/TrainModule_{}".format(i + 1))
  
  writer.close()
  # 结束时间
  end_time = time.time()
  print(end_time - start_time)
  ```





#### 方法二：device("cuda")



- 学习代码实例

  ```python
  # 选择设备
  # 可选择不同参数更换加速设备
  device = torch.device("cuda" if torch.cuda.is_available() else "cpu")	# 适应各种环境的写法
  device1 = torch.device("cuda")
  device2 = torch.device("cuda:0")  # 写法与上相同
  device3 = torch.device("cpu")
  
  # 创建网络模型
  trainModule = TrainModule()
  trainModule = trainModule.to(device1)  # 使用 GPU 加速
  # trainModule.to(device1)	可简写
  
  # 损失函数
  loss_fun = nn.CrossEntropyLoss()
  loss_fun = loss_fun.to(device1)  # 使用 GPU 加速
  # loss_fun.to(device1)		可简写
  
  # 数据，数据不能简写
  imgs, targets = data
  imgs = imgs.to(device1)  # 使用 GPU 加速
  targets = targets.to(device1)  # 使用 GPU 加
  ```

  





---



### `google`  GPU  训练 

每周免费使用 3 小时

- 打开Google搜索引擎搜索    google colab    进入

- 新建笔记本 **该环境中默认有 pytorch**，

  - 输入命令可以测试环境

    ```python
    import torch
    print(torch.__version__)
    ```

- 测试 GPU是否可用

  - 输入下面代码测试环境

    ```python
    print(torch.cuda.is_available())
    
    # 输出 False，表示不可使用 GPU
    ```

  - 更改环境使用 GPU

    - 登录Google账号
    - 导航栏选择：修改--->笔记本设置--->硬件加速器选择GPU

- 查看 GPU 型号

  ```python
  !nvidia-smi
  ```

  